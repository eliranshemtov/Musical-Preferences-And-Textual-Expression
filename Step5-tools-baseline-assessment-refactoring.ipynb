{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Assessment\n",
    "\n",
    "After applying the few \"state of the art\" tools and gathering their predictions on golden-standard datasets (essays & myPersonality), We'll assess result's accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize The Results\n",
    "\n",
    "Combine predictions from various tools and assess\n",
    "\n",
    "First, we manually combined the results to speadsheets, against the true labels.\n",
    "The assessment spreadsheets are available here:\n",
    "\n",
    "- [Essays dataset](https://github.com/eliranshemtov/Musical-Preferences-And-Textual-Expression/blob/main/analysis/tools-baseline/essays-combined-predictions.xlsx)\n",
    "- [MyPersonality dataset](https://github.com/eliranshemtov/Musical-Preferences-And-Textual-Expression/blob/main/analysis/tools-baseline/myPersonality-combined-predictions.xlsx)\n",
    "- [MyPersonality concatenated dataset](https://github.com/eliranshemtov/Musical-Preferences-And-Textual-Expression/blob/main/analysis/tools-baseline/myPersonality-concatenated-combined-predictions.xlsx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score\n",
    "\n",
    "Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right. Formally, accuracy has the following definition:\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mtext>Accuracy</mtext>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mtext>Number of correct predictions</mtext>\n",
    "    <mtext>Total number of predictions</mtext>\n",
    "  </mfrac>\n",
    "</math>\n",
    "\n",
    "For binary classification, accuracy can also be calculated in terms of positives and negatives as follows:\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "<mtext>Accuracy</mtext>\n",
    "<mo>=</mo>\n",
    "<mfrac>\n",
    "<mrow>\n",
    "<mi>T</mi>\n",
    "<mi>P</mi>\n",
    "<mo>+</mo>\n",
    "<mi>T</mi>\n",
    "<mi>N</mi>\n",
    "</mrow>\n",
    "<mrow>\n",
    "<mi>T</mi>\n",
    "<mi>P</mi>\n",
    "<mo>+</mo>\n",
    "<mi>T</mi>\n",
    "<mi>N</mi>\n",
    "<mo>+</mo>\n",
    "<mi>F</mi>\n",
    "<mi>P</mi>\n",
    "<mo>+</mo>\n",
    "<mi>F</mi>\n",
    "<mi>N</mi>\n",
    "</mrow>\n",
    "</mfrac>\n",
    "</math>\n",
    "\n",
    "Where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Example:\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Accuracy Score** per trait (out of the Big Five), per tool (out of the 3 we used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Normalization Functions ###\n",
    "###############################\n",
    "\n",
    "\n",
    "def convert_yn_to_01(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Used by True Labels (Convert Y/N labels to 0/1)\n",
    "    \"\"\"\n",
    "    return np.where(scores == \"y\", 1, 0)\n",
    "\n",
    "\n",
    "def normalize_scores_by_half_threshold(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Used by Tool #3, #1 & #4\n",
    "    \"\"\"\n",
    "    return np.where(scores >= 0.5, 1, 0)\n",
    "\n",
    "\n",
    "def re_normalize_tool_1(column_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The original normalization logic done by the actual tool is as follows and does not yield meaningful results because the prediction for OPN and NEU are constantly the max and min values.\n",
    "        def original_tool_normalization():\n",
    "            min_value = min(pred_sOPN, pred_sCON, pred_sEXT, pred_sAGR, pred_sNEU)\n",
    "            max_value = max(pred_sOPN, pred_sCON, pred_sEXT, pred_sAGR, pred_sNEU)\n",
    "\n",
    "            scaled_min = 0.05\n",
    "            scaled_max = 0.95\n",
    "\n",
    "            pred_sOPN_normalized = (pred_sOPN - min_value) / (max_value - min_value) * (scaled_max - scaled_min) + scaled_min  # Always scores to 0.95\n",
    "            pred_sCON_normalized = (pred_sCON - min_value) / (max_value - min_value) * (scaled_max - scaled_min) + scaled_min\n",
    "            pred_sEXT_normalized = (pred_sEXT - min_value) / (max_value - min_value) * (scaled_max - scaled_min) + scaled_min\n",
    "            pred_sAGR_normalized = (pred_sAGR - min_value) / (max_value - min_value) * (scaled_max - scaled_min) + scaled_min\n",
    "            pred_sNEU_normalized = (pred_sNEU - min_value) / (max_value - min_value) * (scaled_max - scaled_min) + scaled_min  # Always scores to 0.5\n",
    "    \"\"\"\n",
    "    scaled_min = 0.05\n",
    "    scaled_max = 0.95\n",
    "\n",
    "    min_value = column_df.min()\n",
    "    max_value = column_df.max()\n",
    "\n",
    "    return scaled_min + (column_df - min_value) / (max_value - min_value) * (\n",
    "        scaled_max - scaled_min\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Init PredictionLoader per tool ###\n",
    "######################################\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictionsLoader:\n",
    "    columns: list[str]\n",
    "    normalization_functions: list[callable]\n",
    "    predictions: pd.DataFrame = None\n",
    "    accuracy: list[float] = None\n",
    "\n",
    "    def load(self, df: pd.DataFrame) -> None:\n",
    "        self.predictions = pd.DataFrame()\n",
    "        for col in self.columns:\n",
    "            for norm_func in self.normalization_functions:\n",
    "                df[col] = norm_func(df[col])\n",
    "            self.predictions = pd.concat([self.predictions, df[col]], axis=1)\n",
    "\n",
    "    def calc_accuracy(self, truth: pd.DataFrame) -> None:\n",
    "        self.accuracy = []\n",
    "        for i in range(len(self.columns)):\n",
    "            self.accuracy.append(\n",
    "                accuracy_score(\n",
    "                    truth.predictions[truth.columns[i]],\n",
    "                    self.predictions[self.columns[i]],\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "truth = PredictionsLoader(\n",
    "    [\"cEXT\", \"cNEU\", \"cAGR\", \"cCON\", \"cOPN\"], [convert_yn_to_01])\n",
    "\n",
    "tool1 = PredictionsLoader(\n",
    "    [\"pred_sEXT\", \"pred_sNEU\", \"pred_sAGR\", \"pred_sCON\", \"pred_sOPN\"],\n",
    "    [re_normalize_tool_1, normalize_scores_by_half_threshold],\n",
    ")\n",
    "\n",
    "tool3 = PredictionsLoader(\n",
    "    [\n",
    "        \"BIG5_Extraversion\",\n",
    "        \"BIG5_Neuroticism\",\n",
    "        \"BIG5_Agreeableness\",\n",
    "        \"BIG5_Conscientiousness\",\n",
    "        \"BIG5_Openness\",\n",
    "    ],\n",
    "    [normalize_scores_by_half_threshold],\n",
    ")\n",
    "\n",
    "tool4 = PredictionsLoader(\n",
    "    [\n",
    "        \"cEXT_prediction\",\n",
    "        \"cNEU_prediction\",\n",
    "        \"cAGR_prediction\",\n",
    "        \"cCON_prediction\",\n",
    "        \"cOPN_prediction\",\n",
    "    ],\n",
    "    [normalize_scores_by_half_threshold],\n",
    ")\n",
    "\n",
    "loaders = [truth, tool1, tool3, tool4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_measure_accuracy(input_file_path: str, output_file_path: str):\n",
    "    df = pd.read_excel(input_file_path, header=1)\n",
    "    for loader in loaders:\n",
    "        loader.load(df)\n",
    "        loader.calc_accuracy(truth)\n",
    "        print(\"Accuracy:\", loader.accuracy)\n",
    "    csv_headers = [\"Tool\"] + truth.columns\n",
    "    tool1.accuracy.insert(0, \"Tool #1\")\n",
    "    tool3.accuracy.insert(0, \"Tool #3\")\n",
    "    tool4.accuracy.insert(0, \"Tool #4\")\n",
    "    pd.DataFrame(\n",
    "        [tool1.accuracy, tool3.accuracy, tool4.accuracy], columns=csv_headers\n",
    "    ).to_csv(output_file_path, index=False)\n",
    "    print(\"Accuracy results saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tool's predictions & measure accuracy - Essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Accuracy: [0.5214748784440842, 0.5040518638573744, 0.5072933549432739, 0.5113452188006483, 0.5672609400324149]\n",
      "Accuracy: [0.4959481361426256, 0.549837925445705, 0.5101296596434359, 0.5388978930307942, 0.5575364667747164]\n",
      "Accuracy: [0.5145867098865479, 0.5113452188006483, 0.5121555915721232, 0.5324149108589952, 0.5178282009724473]\n",
      "Accuracy results saved to ./analysis/tools-baseline/essays-accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "load_and_measure_accuracy(\n",
    "    \"./analysis/tools-baseline/essays-combined-predictions.xlsx\",\n",
    "    \"./analysis/tools-baseline/essays-accuracy.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote!\n",
      "cEXT 0.5190437601296597\n",
      "cNEU 0.5243111831442464\n",
      "cAGR 0.5178282009724473\n",
      "cCON 0.5478119935170178\n",
      "cOPN 0.5611831442463533\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\n",
    "    \"./analysis/tools-baseline/essays-combined-predictions.xlsx\", header=1\n",
    ")\n",
    "\n",
    "result = {k: [] for k in truth.columns}\n",
    "for i in range(len(tool1.predictions)):\n",
    "    for col in range(5):\n",
    "        val = (\n",
    "            tool1.predictions[tool1.columns[col]][i]\n",
    "            + tool3.predictions[tool3.columns[col]][i]\n",
    "            + tool4.predictions[tool4.columns[col]][i]\n",
    "        )\n",
    "        result[truth.columns[col]].append(1 if val > 1 else 0)\n",
    "\n",
    "print(\"Majority Vote!\")\n",
    "for col in range(5):\n",
    "    print(\n",
    "        truth.columns[col],\n",
    "        accuracy_score(\n",
    "            truth.predictions[truth.columns[col]], result[truth.columns[col]]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load tool's predictions & measure accuracy - myPersonality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Accuracy: [0.652, 0.812, 0.608, 0.688, 0.844]\n",
      "Accuracy: [0.596, 0.56, 0.568, 0.572, 0.544]\n",
      "Accuracy: [0.424, 0.396, 0.524, 0.448, 0.684]\n",
      "Accuracy results saved to ./analysis/tools-baseline/myPersonality-accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "load_and_measure_accuracy(\n",
    "    \"./analysis/tools-baseline/myPersonality-concatenated-combined-predictions.xlsx\",\n",
    "    \"./analysis/tools-baseline/myPersonality-accuracy.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote!\n",
      "cEXT 0.624\n",
      "cNEU 0.644\n",
      "cAGR 0.596\n",
      "cCON 0.612\n",
      "cOPN 0.78\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\n",
    "    \"./analysis/tools-baseline/myPersonality-concatenated-combined-predictions.xlsx\",\n",
    "    header=1,\n",
    ")\n",
    "\n",
    "result = {k: [] for k in truth.columns}\n",
    "for i in range(len(tool1.predictions)):\n",
    "    for col in range(5):\n",
    "        val = (\n",
    "            tool1.predictions[tool1.columns[col]][i]\n",
    "            + tool3.predictions[tool3.columns[col]][i]\n",
    "            + tool4.predictions[tool4.columns[col]][i]\n",
    "        )\n",
    "        result[truth.columns[col]].append(1 if val > 1 else 0)\n",
    "\n",
    "print(\"Majority Vote!\")\n",
    "for col in range(5):\n",
    "    print(\n",
    "        truth.columns[col],\n",
    "        accuracy_score(\n",
    "            truth.predictions[truth.columns[col]], result[truth.columns[col]]\n",
    "        ),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
