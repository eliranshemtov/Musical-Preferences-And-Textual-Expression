{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forming Personality Traits Baseline\n",
    "\n",
    "## By assessing existing tools' predictions\n",
    "\n",
    "The data that will be assessed in this section will be: [essays.zip](\"http://web.archive.org/web/20160519045708/http://mypersonality.org/wiki/lib/exe/fetch.php?media=wiki:essays.zip\"): a large dataset of 2400 stream-of-consciousness texts labelled with personality, produced by Pennebaker & King 1999 and used by Mairesse et al. 2007.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool #1 - https://project.fuguixing.me/\n",
    "\n",
    "A web application for personality analysis, Big Five personality prediction, and emotion analysis. Powered by Azure Static Web App, Azure Function, React, and Machine Learning\n",
    "Sourced at: https://github.com/fuguixing/psychology-insights-frontend/tree/master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2468it [00:02, 1171.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with added columns created: ./analysis/tool-1-baseline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_url = \"https://project.fuguixing.me/api/bigfive\"\n",
    "essays_data_csv_file_path = \"./data/essays.csv\"\n",
    "result_file_path = \"./analysis/tool-1-baseline.csv\"\n",
    "headers = {\"Content-Type\": \"text/plain\"}\n",
    "\n",
    "@retry((requests.exceptions.RequestException, requests.exceptions.Timeout, requests.exceptions.HTTPError, requests.exceptions.ConnectionError, requests.exceptions.JSONDecodeError), delay=5, tries=5)\n",
    "def throttle_post_request(url, payload):\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    if response.status_code !== 200:\n",
    "        raise requests.exceptions.HTTPError(f\"POST request failed. Status code: {response.status_code}. Identifier: {identifier}\")\n",
    "    return response.json()\n",
    "\n",
    "with open(essays_data_csv_file_path, newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "    fieldnames = csv_reader.fieldnames + [\n",
    "        \"pred_sOPN\",\n",
    "        \"pred_sOPN_normalized\",\n",
    "        \"pred_sCON\",\n",
    "        \"pred_sCON_normalized\",\n",
    "        \"pred_sEXT\",\n",
    "        \"pred_sEXT_normalized\",\n",
    "        \"pred_sAGR\",\n",
    "        \"pred_sAGR_normalized\",\n",
    "        \"pred_sNEU\",\n",
    "        \"pred_sNEU_normalized\",\n",
    "        \"pred_sentiment\",\n",
    "    ]\n",
    "\n",
    "    with open(result_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as updated_csvfile:\n",
    "        csv_writer = csv.DictWriter(updated_csvfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        for row in tqdm(csv_reader):\n",
    "            payload = json.dumps(f'\"{row.get(\"TEXT\")}\"')\n",
    "            new_values = {}\n",
    "            api_data= throttle_post_request(api_url, payload)\n",
    "            prediction = api_data.get(\"prediction\", {})\n",
    "            new_values.update(prediction)\n",
    "            row.update(new_values)\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file with added columns created: {result_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### myPersonality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9917it [1:19:10,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with added columns created: ./analysis/tool-1-baseline-myPersonality-2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from retry import retry\n",
    "\n",
    "api_url = \"https://project.fuguixing.me/api/bigfive\"\n",
    "essays_data_csv_file_path = \"./data/myPersonality.csv\"\n",
    "result_file_path = \"./analysis/tool-1-baseline-myPersonality-2.csv\"\n",
    "headers = {\"Content-Type\": \"text/plain\"}\n",
    "\n",
    "\n",
    "@retry(\n",
    "    (\n",
    "        requests.exceptions.RequestException,\n",
    "        requests.exceptions.Timeout,\n",
    "        requests.exceptions.HTTPError,\n",
    "        requests.exceptions.ConnectionError,\n",
    "        requests.exceptions.JSONDecodeError,\n",
    "    ),\n",
    "    delay=5,\n",
    "    tries=7,\n",
    ")\n",
    "def throttle_post_request(url, payload, identifier):\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise requests.exceptions.HTTPError(\n",
    "            f\"POST request failed. Status code: {response.status_code}. Identifier: {identifier}\"\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "with open(essays_data_csv_file_path, newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "    fieldnames = csv_reader.fieldnames + [\n",
    "        \"pred_sOPN\",\n",
    "        \"pred_sOPN_normalized\",\n",
    "        \"pred_sCON\",\n",
    "        \"pred_sCON_normalized\",\n",
    "        \"pred_sEXT\",\n",
    "        \"pred_sEXT_normalized\",\n",
    "        \"pred_sAGR\",\n",
    "        \"pred_sAGR_normalized\",\n",
    "        \"pred_sNEU\",\n",
    "        \"pred_sNEU_normalized\",\n",
    "        \"pred_sentiment\",\n",
    "    ]\n",
    "\n",
    "    with open(result_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as updated_csvfile:\n",
    "        csv_writer = csv.DictWriter(updated_csvfile, fieldnames=fieldnames)\n",
    "        csv_writer.writeheader()\n",
    "        counter = 1\n",
    "        for row in tqdm(csv_reader):\n",
    "            if counter < 3552:\n",
    "                counter += 1\n",
    "                continue\n",
    "            payload = json.dumps(f'\"{row.get(\"STATUS\")}\"')\n",
    "            new_values = {}\n",
    "            api_data = throttle_post_request(api_url, payload, row.get(\"ID\"))\n",
    "            prediction = api_data.get(\"prediction\", {})\n",
    "            new_values.update(prediction)\n",
    "            row.update(new_values)\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "\n",
    "print(f\"CSV file with added columns created: {result_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool #2 - [Personality Recognizer v1.03](http://farm2.user.srcf.net/research/personality/recognizer.html)\n",
    "\n",
    "# ⚠️ Aborted. Poor & Outdated results\n",
    "\n",
    "This work is a bit old [Mairesse et al., 2007](http://farm2.user.srcf.net/research/papers/personality-jair07.pdf), however, acts as a real black-box.\n",
    "This Java program is based on models analyzed in the paper, and shown to predict personality scores significantly better than a constant baseline. The program uses a command line interface, and outputs scores on a scale from 1 to 7, e.g. where 7 is strongly extravert.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll parse the essays.csv dataset and convert it to a folder of txt files, as this program expects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "csv_file_path = \"./data/essays.csv\"\n",
    "output_folder = \"./data/essays_as_txt\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "with open(csv_file_path, newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=\",\", quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        auth_id = row.get(\"#AUTHID\", \"\")\n",
    "        text_data = row.get('\"TEXT\"', \"\")\n",
    "\n",
    "        if auth_id and text_data:\n",
    "            txt_file_path = os.path.join(output_folder, f\"{auth_id}.txt\")\n",
    "\n",
    "            with open(txt_file_path, \"w\", encoding=\"utf-8\") as txtfile:\n",
    "                txtfile.write(text_data)\n",
    "\n",
    "print(\"TXT files saved in the output folder:\", output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As this package is old and un-maintained, it requires significant effort to run it. The assessment results for the essays.csv dataset is detailed in the original [article](http://farm2.user.srcf.net/research/papers/personality-jair07.pdf) (page 19/44)\n",
    "\n",
    "\"Classification results for the essays corpus with self-reports are in Table 12. Interestingly,\n",
    "openness to experience is the easiest trait to model as five classifiers out of six significantly\n",
    "outperform the baseline and four of them produce their best performance for that trait,\n",
    "with accuracies up to 62.1% using support vector machines (SMO). Emotional stability\n",
    "produces the second best performance for four classifiers out of six, with 57.4% accuracy\n",
    "for the SMO model. Conscientiousness is the hardest trait to model as only two classifiers\n",
    "significantly outperform the baseline, however the SMO model performs as well as the best\n",
    "model for extraversion and agreeableness, with around 55% correct classifications.\n",
    "We find that support vector machines generally perform the best, with Naive Bayes and\n",
    "AdaboostM1 in second position. SMO significantly outperforms the majority class baseline\n",
    "for each trait. A J48 decision tree for recognising extraversion is shown in Figure 1, and the\n",
    "rule-based JRip model classifying openness to experience with 58.8% accuracy is illustrated\n",
    "in Table 16.\"\n",
    "\n",
    "We can try and utilize this Java app if the results looks interesting enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool #3 Apply Magic Sauce\n",
    "\n",
    "Apply Magic Sauce is a non-profit academic research project coordinated by the University of Cambridge Psychometrics Centre.\n",
    "The project aimed to analyze and predict individuals' psychological traits, such as personality, based on their digital footprints, including social media activity, likes, and other online behaviors. The project utilized advanced algorithms and machine learning techniques to make predictions about users' personalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2468it [51:59,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with added columns created: ./analysis/tool-3-baseline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from calendar import c\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_url = \"https://applymagicsauce.com/api/predictions/text\"\n",
    "api_token = \"\"\n",
    "essays_data_csv_file_path = \"./data/essays.csv\"\n",
    "result_file_path = \"./analysis/tool-3-baseline.csv\"\n",
    "\n",
    "predictions_fields = [\n",
    "    \"BIG5_Openness\",\n",
    "    \"BIG5_Conscientiousness\",\n",
    "    \"BIG5_Extraversion\",\n",
    "    \"BIG5_Agreeableness\",\n",
    "    \"BIG5_Neuroticism\",\n",
    "    \"Female\",\n",
    "    \"Age\",\n",
    "]\n",
    "with open(essays_data_csv_file_path, newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "    fieldnames = csv_reader.fieldnames + predictions_fields\n",
    "\n",
    "    with open(result_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as updated_csvfile:\n",
    "        csv_writer = csv.DictWriter(\n",
    "            updated_csvfile, fieldnames=fieldnames, delimiter=\",\"\n",
    "        )\n",
    "        csv_writer.writeheader()\n",
    "        count = 0\n",
    "        for row in tqdm(csv_reader):\n",
    "            if count < 1922:\n",
    "                count += 1\n",
    "                continue\n",
    "            payload = f'\"{row.get(\"TEXT\")}\"'\n",
    "            headers = {\n",
    "                \"Content-Type\": \"text/plain\",\n",
    "                \"Authorization\": f\"Bearer {api_token}\",\n",
    "            }\n",
    "            response = requests.post(api_url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                api_data = response.json()\n",
    "                prediction = {\n",
    "                    i.get(\"trait\"): i.get(\"value\") for i in api_data.get(\"predictions\")\n",
    "                }\n",
    "                new_values.update(prediction)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"POST request failed for row with ID {row['#AUTHID']}. Status code: {response.status_code}\"\n",
    "                )\n",
    "\n",
    "            row.update(new_values)\n",
    "            count += 1\n",
    "            try:\n",
    "                del row[\"trait\"]\n",
    "            except:\n",
    "                pass\n",
    "            csv_writer.writerow(row)\n",
    "            time.sleep(1)\n",
    "\n",
    "print(f\"CSV file with added columns created: {result_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MyPersonality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2468it [51:59,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with added columns created: ./analysis/tool-3-baseline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from calendar import c\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_url = \"https://applymagicsauce.com/api/predictions/text\"\n",
    "api_token = \"\"\n",
    "essays_data_csv_file_path = \"./data/myPersonality.csv\"\n",
    "result_file_path = \"./analysis/tool-3-baseline-myPersonality.csv\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/plain\",\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "}\n",
    "\n",
    "\n",
    "@retry(\n",
    "    (\n",
    "        requests.exceptions.RequestException,\n",
    "        requests.exceptions.Timeout,\n",
    "        requests.exceptions.HTTPError,\n",
    "        requests.exceptions.ConnectionError,\n",
    "        requests.exceptions.JSONDecodeError,\n",
    "    ),\n",
    "    delay=5,\n",
    "    tries=3,\n",
    ")\n",
    "def throttle_post_request(url, payload, identifier):\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise requests.exceptions.HTTPError(\n",
    "            f\"POST request failed. Status code: {response.status_code}. Identifier: {identifier}\"\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "predictions_fields = [\n",
    "    \"BIG5_Openness\",\n",
    "    \"BIG5_Conscientiousness\",\n",
    "    \"BIG5_Extraversion\",\n",
    "    \"BIG5_Agreeableness\",\n",
    "    \"BIG5_Neuroticism\",\n",
    "    \"Female\",\n",
    "    \"Age\",\n",
    "]\n",
    "\n",
    "with open(essays_data_csv_file_path, newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "    csv_reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "    fieldnames = csv_reader.fieldnames + predictions_fields\n",
    "\n",
    "    with open(result_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as updated_csvfile:\n",
    "        csv_writer = csv.DictWriter(\n",
    "            updated_csvfile, fieldnames=fieldnames, delimiter=\",\"\n",
    "        )\n",
    "        csv_writer.writeheader()\n",
    "        for row in tqdm(csv_reader):\n",
    "            payload = json.dumps(f'\"{row.get(\"STATUS\")}\"')\n",
    "            api_data = throttle_post_request(api_url, payload, identifier)\n",
    "            prediction = {\n",
    "                i.get(\"trait\"): i.get(\"value\") for i in api_data.get(\"predictions\")\n",
    "            }\n",
    "            new_values.update(prediction)\n",
    "            row.update(new_values)\n",
    "            try:\n",
    "                del row[\"trait\"]\n",
    "            except:\n",
    "                pass\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file with added columns created: {result_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool #4 [Personality Prediction From Text](https://github.com/jkwieser/personality-prediction-from-text/tree/master)\n",
    "\n",
    "[Personality Prediction From Text](https://github.com/jkwieser/personality-prediction-from-text/tree/master) project is an opensource project insipired by this paper:\n",
    "[Majumder, N., Poria, S., Gelbukh, A., & Cambria, E. (2017). Deep Learning-Based Document Modeling for Personality Detection from Text. IEEE Intelligent Systems, 32(2), 74-79. https://doi.org/10.1109/MIS.2017.23](https://doi.org/10.1109/MIS.2017.23)\n",
    "\n",
    "This project was last maintained at 2020 and requires outdated libraries and hardware. Therefore I had to load it in docker containerized environment (based on the publicly available docker image [jbei/scikit-learn:21.03](https://hub.docker.com/r/jbei/scikit-learn)) to simulate the required conditions.\n",
    "\n",
    "This project models' were trained on the essays dataset and some more (emotion_lexicon, mbti_1 & typed_comments), and utilizes various techniques:\n",
    "\n",
    "- Classification\n",
    "- SVM (sklearn)\n",
    "- Decision Tree (sklearn)\n",
    "- Naive Bayes (sklearn)\n",
    "- Logistic Regression (sklearn)\n",
    "- Random Forest (sklearn)\n",
    "- Feature extraction\n",
    "- Bags of Words (sklearn CountVectorizer)\n",
    "- GloVe pretrained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essays & MyPersonality\n",
    "\n",
    "Note that this code was executed in docker containerized environment (based on the publicly available docker image [jbei/scikit-learn:21.03](https://hub.docker.com/r/jbei/scikit-learn)) to simulate the required conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "code_path = \"../personality-prediction-from-text/\"\n",
    "cEXT = pickle.load(open(f\"{code_path}/data/models/cEXT.p\", \"rb\"))\n",
    "cNEU = pickle.load(open(f\"{code_path}/data/models/cNEU_new.pkl\", \"rb\"))\n",
    "cAGR = pickle.load(open(f\"{code_path}/data/models/cAGR.p\", \"rb\"))\n",
    "cCON = pickle.load(open(f\"{code_path}/data/models/cCON.p\", \"rb\"))\n",
    "cOPN = pickle.load(open(f\"{code_path}/data/models/cOPN.p\", \"rb\"))\n",
    "vectorizer_31 = pickle.load(open(f\"{code_path}/data/models/vectorizer_31.p\", \"rb\"))\n",
    "vectorizer_30 = pickle.load(open(f\"{code_path}/data/models/vectorizer_30.p\", \"rb\"))\n",
    "\n",
    "\n",
    "def predict_personality(text):\n",
    "    scentences = re.split(\"(?<=[.!?]) +\", text)\n",
    "    text_vector_31 = vectorizer_31.transform(scentences)\n",
    "    text_vector_30 = vectorizer_30.transform(scentences)\n",
    "    EXT = cEXT.predict(text_vector_31)\n",
    "    NEU = cNEU.predict(text_vector_30)\n",
    "    AGR = cAGR.predict(text_vector_31)\n",
    "    CON = cCON.predict(text_vector_31)\n",
    "    OPN = cOPN.predict(text_vector_31)\n",
    "    return [EXT[0], NEU[0], AGR[0], CON[0], OPN[0]]\n",
    "\n",
    "\n",
    "def essays_predictions():\n",
    "    input_csv_path = f\"{code_path}/data/training/essays.csv\"\n",
    "    output_csv_path = \"./data/tool-4-baseline-essays.csv\"\n",
    "    with open(input_csv_path, \"r\", newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        fieldnames = reader.fieldnames + [\n",
    "            \"cEXT_prediction\",\n",
    "            \"cNEU_prediction\",\n",
    "            \"cAGR_prediction\",\n",
    "            \"cCON_prediction\",\n",
    "            \"cOPN_prediction\",\n",
    "        ]\n",
    "\n",
    "        with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as output_csvfile:\n",
    "            writer = csv.DictWriter(output_csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in tqdm(reader):\n",
    "                text_to_predict = row[\"TEXT\"]\n",
    "                prediction_result = predict_personality(text_to_predict)\n",
    "                row[\"cEXT_prediction\"] = prediction_result[0]\n",
    "                row[\"cNEU_prediction\"] = prediction_result[1]\n",
    "                row[\"cAGR_prediction\"] = prediction_result[2]\n",
    "                row[\"cCON_prediction\"] = prediction_result[3]\n",
    "                row[\"cOPN_prediction\"] = prediction_result[4]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(\"Prediction results appended to the original CSV file.\")\n",
    "\n",
    "\n",
    "def myPersonality_predictions():\n",
    "    input_csv_path = f\"{code_path}/data/training/myPersonality.csv\"\n",
    "    output_csv_path = \"./data/tool-4-baseline-myPersonality.csv\"\n",
    "    with open(input_csv_path, \"r\", newline=\"\", encoding=\"ISO-8859-1\") as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        fieldnames = reader.fieldnames + [\n",
    "            \"cEXT_prediction\",\n",
    "            \"cNEU_prediction\",\n",
    "            \"cAGR_prediction\",\n",
    "            \"cCON_prediction\",\n",
    "            \"cOPN_prediction\",\n",
    "        ]\n",
    "\n",
    "        with open(output_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as output_csvfile:\n",
    "            writer = csv.DictWriter(output_csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in tqdm(reader):\n",
    "                text_to_predict = row[\"STATUS\"]\n",
    "                prediction_result = predict_personality(text_to_predict)\n",
    "                row[\"cEXT_prediction\"] = prediction_result[0]\n",
    "                row[\"cNEU_prediction\"] = prediction_result[1]\n",
    "                row[\"cAGR_prediction\"] = prediction_result[2]\n",
    "                row[\"cCON_prediction\"] = prediction_result[3]\n",
    "                row[\"cOPN_prediction\"] = prediction_result[4]\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(\"Prediction results appended to the original CSV file.\")\n",
    "\n",
    "\n",
    "myPersonality_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
